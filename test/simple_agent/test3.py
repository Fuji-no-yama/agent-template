from agent_template import Agent, BaseTool, OpenAILLM, tool


class DataProcessingTool(BaseTool):
    @tool()
    def analyze_scores(self, scores: list[int]) -> dict[str, float]:
        """
        ã‚¹ã‚³ã‚¢ã®ãƒªã‚¹ãƒˆã‚’åˆ†æã—ã€çµ±è¨ˆæƒ…å ±ã‚’è¿”ã™ãƒ„ãƒ¼ãƒ«

        Args:
            scores (list[int]): åˆ†æå¯¾è±¡ã®ã‚¹ã‚³ã‚¢ã®ãƒªã‚¹ãƒˆ

        Returns:
            dict[str, float]: çµ±è¨ˆæƒ…å ±ï¼ˆå¹³å‡ã€æœ€å¤§ã€æœ€å°ï¼‰
        """
        if not scores:
            return {"average": 0.0, "max": 0.0, "min": 0.0}

        return {
            "average": sum(scores) / len(scores),
            "max": float(max(scores)),
            "min": float(min(scores)),
        }

    @tool()
    def process_student_data(self, student_info: dict[str, int]) -> str:
        """
        å­¦ç”Ÿã®æƒ…å ±ã‚’å‡¦ç†ã—ã€è©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«

        Args:
            student_info (dict[str, int]): å­¦ç”Ÿã®ç§‘ç›®åˆ¥ã‚¹ã‚³ã‚¢è¾æ›¸

        Returns:
            str: è©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆ
        """
        if not student_info:
            return "å­¦ç”Ÿãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

        total_score = sum(student_info.values())
        subject_count = len(student_info)
        average = total_score / subject_count

        best_subject = max(student_info, key=student_info.get)
        worst_subject = min(student_info, key=student_info.get)

        return (
            f"ç·åˆè©•ä¾¡ï¼šå¹³å‡ç‚¹ {average:.1f}ç‚¹\n"
            f"æœ€ã‚‚å¾—æ„ãªç§‘ç›®ï¼š{best_subject}ï¼ˆ{student_info[best_subject]}ç‚¹ï¼‰\n"
            f"æ”¹å–„ãŒå¿…è¦ãªç§‘ç›®ï¼š{worst_subject}ï¼ˆ{student_info[worst_subject]}ç‚¹ï¼‰"
        )

    @tool()
    def get_class_statistics(self, class_data: list[dict[str, int]]) -> dict[str, float]:
        """
        ã‚¯ãƒ©ã‚¹å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ã™ã‚‹ãƒ„ãƒ¼ãƒ«

        Args:
            class_data (list[dict[str, int]]): ã‚¯ãƒ©ã‚¹å…¨ä½“ã®å­¦ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆå„å­¦ç”Ÿã®ç§‘ç›®åˆ¥ã‚¹ã‚³ã‚¢ï¼‰

        Returns:
            dict[str, float]: ã‚¯ãƒ©ã‚¹å…¨ä½“ã®çµ±è¨ˆæƒ…å ±
        """
        if not class_data:
            return {"class_average": 0.0, "total_students": 0.0}

        all_scores = []
        total_students = len(class_data)

        for student_data in class_data:
            all_scores.extend(student_data.values())

        class_average = sum(all_scores) / len(all_scores) if all_scores else 0.0

        return {
            "class_average": class_average,
            "total_students": float(total_students),
            "total_subjects_count": float(len(all_scores)),
        }


if __name__ == "__main__":
    llm = OpenAILLM(model="gpt-4.1", temperature=0.0)
    tools = [DataProcessingTool()]
    agent = Agent(tools=tools, llm=llm, log_dir="./logs")

    system_prompt = (
        "ã‚ãªãŸã¯æ•™è‚²ãƒ‡ãƒ¼ã‚¿åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚"
        "ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’process_student_dataã‚„get_class_statisticsãªã©ã®"
        "åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦å‡¦ç†ã—ã€åˆ†æçµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        "ãƒ‡ãƒ¼ã‚¿ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã¯ã€å¿…ãšãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦å‡¦ç†ã—ã¦ãã ã•ã„ã€‚"
    )

    # ã‚ˆã‚Šæ˜ç¤ºçš„ãªãƒ†ã‚¹ãƒˆ2: dict[str, int]å‹
    print("=== æ˜ç¤ºçš„ãƒ†ã‚¹ãƒˆ: å­¦ç”Ÿãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆdict[str, int]å‹ï¼‰ ===")
    task2_explicit = (
        "ä»¥ä¸‹ã®å­¦ç”Ÿã®æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’ process_student_data ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚\n"
        "å­¦ç”Ÿã®æˆç¸¾ãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸å½¢å¼ï¼‰:\n"
        '{"æ•°å­¦": 85, "è‹±èª": 92, "ç†ç§‘": 78, "ç¤¾ä¼š": 88, "å›½èª": 90}'
    )

    try:
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–™é‡‘ã‚’ãƒªã‚»ãƒƒãƒˆ
        agent.llm.input_token = 0
        agent.llm.output_token = 0

        response2 = agent.execute_task(system_prompt=system_prompt, task=task2_explicit, use_log=False)
        print("Response2:", response2)
        print(f"Fee: ${agent.get_total_fee():.6f}")
    except (ValueError, TypeError, KeyError) as e:
        print(f"Error in explicit test2: {e}")

    print("\n" + "=" * 60 + "\n")

    # ã‚ˆã‚Šæ˜ç¤ºçš„ãªãƒ†ã‚¹ãƒˆ4: list[dict[str, int]]å‹
    print("=== æ˜ç¤ºçš„ãƒ†ã‚¹ãƒˆ: ã‚¯ãƒ©ã‚¹çµ±è¨ˆï¼ˆlist[dict[str, int]]å‹ï¼‰ ===")
    task4_explicit = (
        "ä»¥ä¸‹ã®ã‚¯ãƒ©ã‚¹å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ get_class_statistics ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚\n"
        "ã‚¯ãƒ©ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼‰:\n"
        "[\n"
        '  {"æ•°å­¦": 85, "è‹±èª": 90, "ç†ç§‘": 88},\n'
        '  {"æ•°å­¦": 92, "è‹±èª": 87, "ç†ç§‘": 95},\n'
        '  {"æ•°å­¦": 78, "è‹±èª": 94, "ç†ç§‘": 82}\n'
        "]"
    )

    try:
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–™é‡‘ã‚’ãƒªã‚»ãƒƒãƒˆ
        agent.llm.input_token = 0
        agent.llm.output_token = 0

        response4 = agent.execute_task(system_prompt=system_prompt, task=task4_explicit, use_log=False)
        print("Response4:", response4)
        print(f"Fee: ${agent.get_total_fee():.6f}")
    except (ValueError, TypeError, KeyError) as e:
        print(f"Error in explicit test4: {e}")

    print("\nğŸ‰ æ˜ç¤ºçš„ãƒ†ã‚¹ãƒˆå®Œäº†!")

    # ãƒ„ãƒ¼ãƒ«ã‚¹ã‚­ãƒ¼ãƒã®ç¢ºèª
    print("\n=== ç”Ÿæˆã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚¹ã‚­ãƒ¼ãƒã®ç¢ºèª ===")
    tool_instance = DataProcessingTool()
    tool_info_list = tool_instance.get_tool_information()

    for tool_info in tool_info_list:
        print(f"\nğŸ“‹ ãƒ„ãƒ¼ãƒ«å: {tool_info['name']}")
        for arg in tool_info["args"]:
            print(f"  å¼•æ•° {arg['name']}: {arg['type_info']}")
            # OpenAI APIå½¢å¼ã¸ã®å¤‰æ›çµæœã‚‚è¡¨ç¤º
            converted_schema = llm.convert_type_info_to_schema(arg["type_info"])
            print(f"  â†’ OpenAI Schema: {converted_schema}")
        print("-" * 40)
